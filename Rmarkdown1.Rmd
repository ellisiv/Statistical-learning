---
title: "Compulsory exercise 1"
author: "Ellisiv Steen, Johanne Skogvang, Helene Behrens"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1: Multiple linear regression


```{r lungcap, echo=TRUE}
library(GLMsData)
data("lungcap")
lungcap$Htcm=lungcap$Ht*2.54
modelA = lm(log(FEV) ~ Age + Htcm + Gender + Smoke, data=lungcap)
summary(modelA)

```

**Q1:** The fitted expression for modelA is constructed from the section "Estimate" in the summary. The expression is on the form

$$\textrm{log(FEV)} = \beta_0 + \beta_1 \cdot \textrm{Age} + \beta_2 \cdot \textrm{Htcm} + \beta_3 \cdot \textrm{GenderM} + \beta_4 \cdot \textrm{Smoke} \\
= -1.944 + 0.023\cdot \textrm{Age} + 0.017 \cdot \textrm{Htcm} + 0.029 \cdot \textrm{GenderM} - 0.046 \cdot \textrm{Smoke}$$

**Q2:** In the summary of modelA, there are some terms that need explaining:

 * Estimate: The estimated weights of the covariates in the fitted model. This is interpreted as the increase of the response variable as the covariate increases with 1 unit. Those are the $\beta$s used in Q1. The particular value "Intercept" is the value of the response variable when all covariates have value zero.
 
 * Std.Error: The std.Error or the standard deviation error is the error in the estimates for $\beta$, that is: $\sqrt{\textrm{Var}(\beta\textrm{s})}$
 
 * Residual standard error: $\sqrt{\frac{\textrm{RSS}}{\textrm{degrees of freedom}}}$
 This is the standard deviation of the residuals. The residuals are a measure of the distance between the data points and the fitted model. 
 
 * F-statistic: The F-statistic is a hypothesis test which compares our fitted model to a much simpler alternative model. This model is called a "Intercept only model" which is a linear model with $\beta_0$ as only non-zero weight. The p-value of the F-statistic is the probability that the "Intercept only model" is as good as our fitted model, i.e. the probability that all $\beta$s$=0$ except for $\beta_0$.

**Q3:** The proportion of variability explained by the fitted modelA is represented my the "Multiple R-squared" and is in our case approximately 81%. This means that 81% of the observed variance is explained by our linear model, which is a fairly good score.

**Q4:** At first glance the plot below of the fitted values vs. residuals, appears normally distributed around 0. By further inspection we see that the points are slightly shiftet upwards around Fitted value = 0.8 and there seems to be a slightly higher variance for very high and very low fitted values. It is also an important observation that there seems to be more extreme negative residuals, but there are more, and less extreme, positive residual observations. This last observation is also supported by the Q-Q plot. Here we see a curvature which may imply a slightly left skewed distribution.
The Anderson-Darling normality test also rejects the null hypothesis that the data follows a normal distribution.
```{r, echo=TRUE}
library(ggplot2)
# residuls vs fitted
ggplot(modelA, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. standardized residuals for model A",
       subtitle = deparse(modelA$call))

# qq-plot of residuals
ggplot(modelA, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", 
       title = "Normal Q-Q", subtitle = deparse(modelA$call))

# normality test
library(nortest) 
ad.test(rstudent(modelA))
```

**Q5:** We now introduce a new model, modelB, which is defined below. We see that in modelB, the residuals are not equally distributed around zero and the variance increases for higher fitted values. We would prefere to use modelA when making inference about FEV because the mean value of the residuals in modelA is approximately zero for all fitted values and the variance is stabile. We have observed that none of this is true for modelB.

```{r,eval=TRUE, echo=TRUE}
modelB = lm(FEV ~ Age + Htcm + Gender + Smoke, data=lungcap)
ggplot(modelB, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. standardized residuals for modelB",
       subtitle = deparse(modelB$call))
```

**Q6:** 
We are interessted in finding out if the $\beta_1 = 0$, which means that all observed correlation between Age and FEV are due to chance. The resulting p-value of the hypothesis test is presented in the summary as the covariate's $\text{Pr(>|t|)}_{\text{Age}} = 7.1\cdot 10^{-12}$. This is a very small p-value, it is interpreted as the probability that we observe this correlation given $\beta_1 = 0$ is $7.1 \cdot 10^{-12}$. Considering that we would reject $\text{H}_0$ at $p < 0.05$, we definately reject it in our case. 

**Q7:** A prediction interval for a t-distributed variable is on the form $$\Bigg[ \hat{\beta_1} \pm t_{\frac{\alpha}{2}, n-p-1}\text{SD}(\hat{\beta_1}) \Bigg] = \Bigg[ \hat{\beta_1} \pm t_{0.005,654-4-1}\text{SD}(\hat{\beta_1}) \Bigg] $$  
This is for our model given as:
```{r,eval=TRUE, echo=TRUE}
cat('[',confint(modelA, level=0.99)[2,1],', ',confint(modelA, level=0.99)[2,2],']' )
```
This tells us that there is a 99% chance of that the true value of $\beta_1$ lies between [0.0147, 0.0320].

**Q8:**

```{r,eval=TRUE}
new = data.frame(Age=16, Htcm=170, Gender="M", Smoke=0)

prelog <- predict.lm(modelA, new, interval = "prediction", level = 0.95)

prediction <- exp(prelog[1])
low <- exp(prelog[2])
upp <- exp(prelog[3])

cat('Predicted "forced expiratory colume" (FEV) = ', prediction, '. \n 95% confidence intercal = [',low, ',',upp,']' )
```
A little reasearch shows that normal FEV for males lies between approximately 2 and 5, which means that the information provided by the prediction interval is of little value as the interval is relatively wide.

# Problem 2: Classification 

```{r, echo=TRUE}
library(ggplot2)
library(lattice)
library(class)# for function knn
library(caret)# for confusion matrices

raw = read.csv("https://www.math.ntnu.no/emner/TMA4268/2019v/data/tennis.csv")
M = na.omit(data.frame(y=as.factor(raw$Result),
                       x1=raw$ACE.1-raw$UFE.1-raw$DBF.1, 
                       x2=raw$ACE.2-raw$UFE.2-raw$DBF.2))
set.seed(4268) # for reproducibility
tr = sample.int(nrow(M),nrow(M)/2)
trte=rep(1,nrow(M))
trte[tr]=0
Mdf=data.frame(M,"istest"=as.factor(trte))
```

**Q9:** The mathematical formula for the K-nearest neighbours estimator is
$\hat{y}(x) = \underset{j}{\mathrm{argmax}} \hat{P}(Y = j|X = x_0)$, where
$$\hat{P}(Y = j | X = x_0) = \frac{1}{K} \sum_{i \in \mathcal{N}_0} I(y_i = j)$$ and $j \in \{0,\: 1\}$

**Q10:** 

```{r,eval=TRUE}
k_values = 1:30

test.e <- vector(mode = "numeric", length = 30)
train.e <- vector(mode = "numeric", length = 30)

for(k in k_values){
  test.e[k] = mean(Mdf[-tr,]$y != knn(Mdf[tr,], Mdf[-tr,],Mdf[tr,]$y, k = k, l = 0))
  train.e[k] = mean(Mdf[tr,]$y != knn(Mdf[tr,], Mdf[tr,],Mdf[tr,]$y, k = k, l = 0))
}
qplot(k_values, test.e)+labs(title="Misclassification error for test data")
qplot(k_values, train.e)+labs(title="Misclassification error for training data")

```


```{r, eval=TRUE}
set.seed(0)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop. 
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv", 
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){ 
  sapply(seq_along(idx), function(j) {
    yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
               cl=M[tr[ -idx[[j]] ], 1],
               test=M[tr[ idx[[j]] ], -1], k = k)
    mean(M[tr[ idx[[j]] ], 1] != yhat)
  })
})
```


**Q11:** 

```{r, eval=TRUE}
cv.e <- vector(mode = "numeric", length = 30)
for(i in 1:30){
  cv.e[i] = mean(cv[,i])
}
cv.se <- vector(mode = "numeric", length = 30)
for(i in 1:30){
  for(j in 1:5){
    cv.se[i] = cv.se[i] + (cv[j,i]- cv.e[i]) ** 2
  }
  cv.se[i] = sqrt(1/5 * cv.se[i])
}

k.min <- which.min(cv.e)
```

**Q12:** The bias in a KNN method will increase with an increasing K and this is due to decreasing complexity. A very complex model can fit models which is less or equal in complexity, and that means that when the complexity decreases we are not able to fit complex "true models". This explains why the bias increases for increasing K in general. The variance on the other hand, decreases for increasing K because the estimated models becomes more stable for perturbations in the data set.

```{r,eval=TRUE}
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
     xlab = "Number of neighbors", ylab="Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle=90, length=.03, code=3, col=co[2])
lines(ks, train.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1, col=co)
```

**Q13:** The most important properties of a good value for K, is that it provides a small misclassification error and it should produce a prediction model with low complexity. The strategy is to find the k-value which produces the smallest misclassification error in the 5-fold test and see if we can choose a higher k with not that much worse misclassification error. We accept a misclassification error equal to the error of the best k plus one time its standard deviation, and choose the largest k satisfying this condition.

```{r,eval=TRUE}
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
size = 100
xnew = apply(M[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(M[tr,-1], M[tr,1], k=k, test=grid)
np = 300
par(mar=rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5, 
        xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
        main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(M[1:np,-1], col=factor(M[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"), 
       col=c("red", "black"), pch=1)
box()
```

**Q14:** 

We plot an ROC curve, an Receiver Operating Characteristics curve, which compares the specificity and the sensitivity of the classifier. We define the specificity and sensitivity as 
$$\text{Specificity} = \frac{\text{True Negative}}{\text{Negiative}}\\ \text{Sensitivity} = \frac{\text{True Positive}}{\text{Positive}}$$. 
An ideal model would have both specificity and sensitivity of 1, which would mean that all wins (positives) would be classified as wins, and all losses (negatives) would be classified as losses. We plot the sensitivity against the specificity for different threshold rates, with the sensitivity increasing from zero and the specificity decreasing from one, and we will thus wish for the curve to hug the upper left corner of the plot.
We can think of the threshold as the porbability p,for which we classify the match as won;

The match is classified as won by player 1 if P(Player 1 wins the game) > p.

When we do random guessing, we will expect that the number of wins that are correctly classified increases with a decreasing p, as it means that over all, more matches will be classified as wins. Equivalently, the number of correctly classified losses will decrease. Thus, when p decreases from 1 to 0, the relation between the spesificity and the sensitivity will change accordingly, and we will observe a straight line;
$$\text{Specificity} = 1 - \text{Sensitivity}$$
This will also mean that the AUC = 0.5. 

```{r,eval=TRUE}
K=30
  
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob= ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
# now KNNprob has probability that player 1 wins, for all matches in the test set

library(pROC)
# now you use predictor=KNNprob and response=M[-tr,1] 
# in your call to the function roc in the pROC library-tr,1
#p
M_roc = roc(M[-tr,1], KNNprob, lagacy.axes = TRUE )
M_auc <- auc(M_roc)
M_auc

ggroc(M_roc) + ggtitle("ROC") + xlab("Specificity") + ylab("Sensitivity")

```

**Q15:**

We see that the misclassification error on the test data is lower with $\tilde{y}$ than with $\hat{y}$, and thus we would prefer to use $\tilde{y}$.

```{r,eval=TRUE}
#The code from Q13:
library("caret")

k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
size = 100
xnew = apply(M[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(M[tr,-1], M[tr,1], k=k, test=grid)
np = 300
par(mar=rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5, 
        xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
        main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(M[1:np,-1], col=factor(M[1:np,1]), pch = 1, lwd = 1.5)
abline(0,1, col = "blue")

legend("topleft", c("Player 1 wins", "Player 2 wins"), 
       col=c("red", "black"), pch=1)
box()

#print(Mdf)
#print(Mdf[-tr,1])


#Confusion matrix for test data:
conf_knn <- confusionMatrix(knn(Mdf[tr,],Mdf[-tr,],Mdf[tr,]$y, k = 30, l = 0),Mdf[-tr,]$y)
print(conf_knn)

cat("Misclassification error for knn: ", 1 - conf_knn$overall['Accuracy'],'\n')
cat("Confusion matrix for the argmax classifier \n: \n")

test.y <- M[-tr,1]
test.x1 <- M[-tr,2]
test.x2 <- M[-tr,3]

int_x1 <- strtoi(test.x1)
int_x2 <- strtoi(test.x2)

pred_argmax <- vector(mode = "character", length = length(test.x1))

for (i in 1:length(int_x1)){
  if(int_x1[i] >= int_x2[i]){
    pred_argmax[i] = "1"
    }
  else{
    pred_argmax[i] = "0"
  }
}

#lager vår egen confusion matrix:
TN <- 0
FP <- 0
FN <- 0
TP <- 0

for (j in 1:length(test.x1)){
  if(test.y[j] == "1" && pred_argmax[j] == "1"){
    TP = TP +1
  }
  if(test.y[j] == "1" && pred_argmax[j] == "0"){
    FN = FN + 1
  }
  if(test.y[j] == "0" && pred_argmax[j] == "0"){
    TN = TN + 1
  }
  if(test.y[j] == "0" && pred_argmax[j] == "1"){
    FP = FP + 1
  }
}

argmax_cm <- matrix(c(TN,FP,FN,TP), nrow = 2, dimnames = list(c(0,1),c(0,1)))
argmax_cm

cat("\n Misclassification error for argmax classifier: ",(FP+FN)/(TN+FP+FN+TP))

```


# Problem 3: Bias-variance trade-off 
Let $\bf{x}$ be a $(p+1) \times 1$ vector of covariates (including a constant 1 as the first term). We are considering a regression problem $Y = f(\bf{x})$, where $\text{E}(\bf{\epsilon}) = 0$ and $\text{Var}(\bf{\epsilon}) = \sigma^2$. We assume that the true function is really a linear combination of the observed covariates, which means that the irreducible error only lies in the $\bf{\epsilon}$-term. 

We will consider two estimators for $\bf{\beta}$. The estimators are based on a training set where $\text{X}$ is a $n\times (p-1)$ design matrix and $\textbf{Y}$ is a $n \times 1$ response vector, with independent entries.
Thus $\text{E}(\textbf{Y})=\text{X}\bf{\beta}$ and covariance matrix $\text{Cov}(\textbf{Y})=\sigma^2 \text{I}$

The first estimator $\hat{\beta}$ is the classical least squares estimator, defined as $$\bf{\hat{\beta}} = (\text{X}^T\text{X})^{-1}\text{X}^T\textbf{Y}$$.
**Q16:** 
$$\text{E}(\boldsymbol{\hat{\beta}}) = (\text{X}^T\text{X})^{-1}\text{X}^T\text{E}[\textbf{Y}] = (\text{X}^T\text{X})^{-1}\text{X}^T\text{X} \boldsymbol{\beta} = \boldsymbol{\beta}$$ 
$$\text{Var}(\boldsymbol{\hat{\beta}}) = (\text{X}^T\text{X})^{-1}\text{X}^T \sigma^2 \text{I} \big[ (\text{X}^T\text{X})^{-1}\text{X}^T\big]^T = \sigma^2 (\text{X}^T\text{X})^{-1}\text{X}^T\text{X}(\text{X}^T\text{X})^{-1} = \sigma^2 (\text{X}^T\text{X})^{-1}$$
**Q17:**
$\hat{f}(\mathbf{x_0}) = \bf{x_0}^T\boldsymbol{\hat{\beta}}$, thus:
$$\text{E}[\hat{f}(\mathbf{x_0})] = \mathbf{x_0}^T \text{E}[\boldsymbol{\hat{\beta}}] = \mathbf{x_0}^T \boldsymbol{\beta}$$
and
$$\text{Var}(\hat{f}(\mathbf{x_0})) = \mathbf{x_0}^T\sigma^2(\text{X}^T\text{X})^{-1}\mathbf{x_0}$$
**Q18:** 
$$\text{E}[(Y_0-\hat{f}(\mathbf{x_0}))]=[\text{E}(\hat{f}(\mathbf{x_0})-f(\mathbf{x}_0))]^2+\text{Var}(\hat{f}(\mathbf{x}_0) ) + \text{Var}(\boldsymbol{\epsilon}) = 0 + \sigma^2\mathbf{x_0}T(\text{X}^T\text{X})^{-1}\mathbf{x_0} + \sigma^2 \\
= \sigma^2(\mathbf{x_0}^T(\text{X}^T\text{X})^{-1}\mathbf{x_0} + 1)$$

The second estimator of $\boldsymbol{\beta}$ is the Ridge regression estimator, defined as 

$$\widetilde{\boldsymbol \beta}=(\text{X}^T\text{X}+\lambda \text{I})^{-1}\text{X}^T{\mathbf Y}$$
Where the $\lambda$ is called a regularization parameter. Notice that for $\lambda = 0$, $\boldsymbol{\hat{\beta}}=\boldsymbol{\widetilde{\beta}}$. To simplify, we define $\text{W} = (\text{X}^T\text{X}+\lambda \text{I})^{-1}\text{X}^T$.  
**Q19:** 
$$\text{E}[\boldsymbol{\widetilde{\beta}}] = (\text{X}^T\text{X} + \lambda I)^{-1}\text{X}^T \, \text{E}[\mathbf{Y}] = \text{W}\text{X}\boldsymbol{\beta},$$   
thus the estimator is biased.  
$$\text{Var}(\boldsymbol{\widetilde{\beta}}) = \text{W} \text{Var}(\mathbf{Y})\text{W}^T = \text{W} \sigma^2 I \text{W}^T 
= \sigma \text{W}\text{W}^T \sigma = \sigma(\text{X}^T\text{X} + \lambda I)^{-1}\text{X}^T\text{X}(\text{X}^T\text{X}+\lambda I)^{-1}\sigma$$

**Q20:** $$\text{E}(\widetilde{f}(x_0)) = \text{E}(x_0^T \widetilde{\beta}) = x_0^T\text{E}(\widetilde{\beta})=x_0^T\text{W}\text{X}\beta$$

$$\text{Var}(\widetilde{f}(x_0)) = x_0^T\text{Var}(\widetilde{\beta})x_0 = \sigma x_0^T \text{W} \text{W}^T x_0 \sigma = \sigma (x_0^T\text{W})(x_0^T\text{W})^T\sigma$$

**Q21:** 
$$\text{E}[(Y_0-\widetilde{f}({\bf x}_0))^2]=[\text{E}(\widetilde{f}({\bf x}_0)-f({\bf x}_0)]^2+\text{Var}(\widetilde{f}({\bf x}_0) ) + \text{Var}(\varepsilon) = (x_0^T\text{W}\text{X}\beta - x_0^T\beta)^2 + \sigma (x_0^T\text{W})(x_0^T\text{W})^T\sigma + \sigma^2$$


```{r,echo=TRUE}
values=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/BVtradeoffvalues.dd")
X=values$X
dim(X)
x0=values$x0
dim(x0)
beta=values$beta
dim(beta)
sigma=values$sigma
sigma
```


**Q22:** 

```{r,eval=TRUE}
sqbias=function(lambda,X,x0,beta)
{
  p=dim(X)[2]
  inv=solve(t(X)%*%X+lambda*diag(p))
  W = inv%*%t(X)
  value= (t(x0)%*%W%*%X%*%beta-t(x0)%*%beta)^2
  return(value)
}
thislambda=seq(0,2,length=500)
sqbiaslambda=rep(NA,length(thislambda))
for (i in 1:length(thislambda)) sqbiaslambda[i]=sqbias(thislambda[i],X,x0,beta)
plot(thislambda,sqbiaslambda,col=2,type="l")
```

This does not exactly look as we expected, the squared bias is increasing as we thought, but the dip around $\lambda = 0.5$ was unexpected. The expected increase in bias is due to a damping of the estimated coefficients which leads to lower complexity and thus higher bias.  


**Q23:** 

```{r,eval=TRUE}
variance=function(lambda,X,x0,sigma)
{
  p=dim(X)[2]
  inv=solve(t(X)%*%X+lambda*diag(p))
  W = inv%*%t(X)
  value=sigma*(t(x0)%*%W)%*%t(t(x0)%*%W)*sigma
  return(value)
}
thislambda=seq(0,2,length=500)
variancelambda=rep(NA,length(thislambda))
for (i in 1:length(thislambda)) variancelambda[i]=variance(thislambda[i],X,x0,sigma)
plot(thislambda,variancelambda,col=4,type="l")
```

The variance curve follows our expectations in the sense that it decreases as lambda increases i.e. the complexity decreases.


**Q24:** 

```{r,eval=TRUE}
tot=sqbiaslambda+variancelambda+sigma^2
which.min(tot)
thislambda[which.min(tot)]
plot(thislambda,tot,col=1,type="l",ylim=c(0,max(tot)))
lines(thislambda, sqbiaslambda,col=2)
lines(thislambda, variancelambda,col=4)
lines(thislambda,rep(sigma^2,500),col="orange")
abline(v=thislambda[which.min(tot)],col=3)
```

The optimal value of $\lambda$ is chosen to minimize the black curve, which is the sum of the irriducible error, the variance and the squared bias. The value optimizing this problem is $\lambda = 0.993988$.

