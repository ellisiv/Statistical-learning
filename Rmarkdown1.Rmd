---
title: "Compulsory exercise 1"
author: "Ellisiv Steen, Johanne Skogvang, Helene Behrens"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1: Multiple linear regression


```{r lungcap, echo=FALSE}
library(GLMsData)
data("lungcap")
lungcap$Htcm=lungcap$Ht*2.54
modelA = lm(log(FEV) ~ Age + Htcm + Gender + Smoke, data=lungcap)
summary(modelA)

```

**Q1:** The fitted expression for modelA is constructed from the section "Estimate" in the summary. The expression is on the form

$$\textrm{log(FEV)} = \beta_0 + \beta_1 \cdot \textrm{Age} + \beta_2 \cdot \textrm{Htcm} + \beta_3 \cdot \textrm{GenderM} + \beta_4 \cdot \textrm{Smoke} \\
= -1.944 + 0.023\cdot \textrm{Age} + 0.017 \cdot \textrm{Htcm} + 0.029 \cdot \textrm{GenderM} - 0.046 \cdot \textrm{Smoke}$$

**Q2:** In the summary of modelA, there are some terms that need explaining:

 * Estimate: The estimated weights of the covariates in the fitted model. Those are the $\beta$s used in Q1. The particular value "Intercept" is the value of the response variable when all covariates have value zero.
 
 * Std.Error: The std.Error or the standard deviation error is the error in the estimates for $\beta$, that is: $\sqrt{\textrm{Var}(\beta\textrm{s})}$
 
 * Residual standard error: $\sqrt{\frac{\textrm{RSS}}{\textrm{degrees of freedom}}}$
 This is the standard deviation of the residuals. The residuals are a measure of the distance between the data points and the fitted model. 
 
 * F-statistic: The F-statistic compares our fitted model to a much simpler alternative model with Intercept as only $\beta$. The p-value is then a hypothesis test where H0 is the hypothesis where all other $\beta$s than $\beta_0 = 0$, that is the "Intercept only model" is equally as good as our fitted model. H1 is then the hypothesis that at least one other $\beta \neq 0$

**Q3:** The proportion of variability explained by the fitted modelA is represented my the "Multiple R-squared" and is in our case approximately 81%.

**Q4:** At first glance at the plot below of the fitted values vs. residuals the points look normally distributed around 0. By further inspection we see that the points are slightly shiftet upwards around 0.8 and there seems to be a slightly higher variance for very high and low fitted values. It is also an important observation that there seems to be more extreme negative residuals, but there are more, and less extreme, positive residual observations. This last observation is also supported by the Q-Q plot. Here we see a curvature which can imply a slightly left skewed distribution.
The Anderson-Darling normality test also rejects the null hypothesis that the data follows a normal distribution.
```{r, echo=TRUE}
library(ggplot2)
# residuls vs fitted
ggplot(modelA, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. standardized residuals for model A",
       subtitle = deparse(modelA$call))

# qq-plot of residuals
ggplot(modelA, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", 
       title = "Normal Q-Q", subtitle = deparse(modelA$call))

# normality test
library(nortest) 
ad.test(rstudent(modelA))
```

**Q5:** We see that modelB, the residuals are not equally distributed around zero and the variance increases for higher fitted values. We would prefere to use modelA when making inference about FEV because the mean value of the residuals in modelA is approxiamtely zero for all fitted values and the variance is stabile. We have observed that none of this is true for modelB.

```{r,eval=TRUE, echo=FALSE}
modelB = lm(FEV ~ Age + Htcm + Gender + Smoke, data=lungcap)
ggplot(modelB, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. standardized residuals for modelB",
       subtitle = deparse(modelB$call))
```

**Q6:** 
If we use the p-value approach 
From the estimation of the coefficients of the linear model it is assumed that they follow a t-distribution. 

**Q7:** 

```{r,eval=TRUE, echo=TRUE}
confint(modelA, level=0.99)
```


**Q8:**

```{r,eval=TRUE}
new = data.frame(Age=16, Htcm=170, Gender="M", Smoke=0)

prelog <- predict.lm(modelA, new, interval = "prediction", level = 0.95)

prediction <- exp(prelog[1])
low <- exp(prelog[2])
upp <- exp(prelog[3])

cat('Predicted "forced expiratory colume" (FEV) = ', prediction, '. \n 95% confidence intercal = [',low, ',',upp,']' )
```

# Problem 2: Classification 

```{r}
library(class)# for function knn
library(caret)# for confusion matrices

raw = read.csv("https://www.math.ntnu.no/emner/TMA4268/2019v/data/tennis.csv")
M = na.omit(data.frame(y=as.factor(raw$Result),
                       x1=raw$ACE.1-raw$UFE.1-raw$DBF.1, 
                       x2=raw$ACE.2-raw$UFE.2-raw$DBF.2))
set.seed(4268) # for reproducibility
tr = sample.int(nrow(M),nrow(M)/2)
trte=rep(1,nrow(M))
trte[tr]=0
Mdf=data.frame(M,"istest"=as.factor(trte))
```

**Q9:** The mathematical formula for the K-nearest neighbours estimator is
$\hat{y}(x) = \max_j \hat{P}(Y = j|X = x_0)$, where
$$\hat{P}(Y = j | X = x_0) = \frac{1}{K} \sum_{i \in \mathcal{N}_0} I(y_i = j)$$

**Q10:** 

```{r,eval=TRUE}
k_values = 1:30

test.e <- vector(mode = "numeric", length = 30)
train.e <- vector(mode = "numeric", length = 30)

for(k in k_values){
  test.e[k] = mean(Mdf[-tr,]$y != knn(Mdf[tr,], Mdf[-tr,],Mdf[tr,]$y, k = k, l = 0))
  train.e[k] = mean(Mdf[tr,]$y != knn(Mdf[tr,], Mdf[tr,],Mdf[tr,]$y, k = k, l = 0))
}
qplot(k_values, test.e)
qplot(k_values, train.e)

```


```{r, eval=FALSE}
set.seed(0)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop. 
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv", 
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){ 
  sapply(seq_along(idx), function(j) {
    yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
               cl=M[tr[ -idx[[j]] ], 1],
               test=M[tr[ idx[[j]] ], -1], k = k)
    mean(M[tr[ idx[[j]] ], 1] != yhat)
  })
})
```


**Q11:** 

```{r, eval=FALSE}
cv.e <- vector(mode = "numeric", length = 30)
for(i in 1:30){
  cv.e[i] = mean(cv[,i])
}
cv.se <- vector(mode = "numeric", length = 30)
for(i in 1:30){
  for(j in 1:5){
    cv.se[i] = cv.se[i] + (cv[j,i]- cv.e[i]) ** 2
  }
  cv.se[i] = sqrt(1/n * cv.se[i])
}

k.min <- which.min(cv.e)
```

**Q12:** 

```{r,eval=FALSE}
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
     xlab = "Number of neighbors", ylab="Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle=90, length=.03, code=3, col=co[2])
lines(ks, train.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1, col=co)
```

**Q13:** 

```{r,eval=FALSE}
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
size = 100
xnew = apply(M[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(M[tr,-1], M[tr,1], k=k, test=grid)
np = 300
par(mar=rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5, 
        xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
        main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(M[1:np,-1], col=factor(M[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"), 
       col=c("red", "black"), pch=1)
box()
```


**Q14:** 

```{r,eval=FALSE}
K=30
  
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob= ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
# now KNNprob has probability that player 1 wins, for all matches in the test set

library(pROC)
# now you use predictor=KNNprob and response=M[-tr,1] 
# in your call to the function roc in the pROC library
```

**Q15:**

```{r,eval=TRUE}
# here you write your code
```


# Problem 3: Bias-variance trade-off 
Let $\bf{x}$ be a $(p+1) \times 1$ vector of covariates (including a constant 1 as the first term). We are considering a regression problem $Y = f(\bf{x})$, where $\text{E}(\bf{\epsilon}) = 0$ and $\text{Var}(\bf{\epsilon}) = \sigma^2$. We assume that the true function is really a linear combination of the observed covariates, which means that the irreducible error only lies in the $\bf{\epsilon}$-term. 

We will consider two estimators for $\bf{\beta}$. The estimators are based on a training set where $\text{X}$ is a $n\times (p-1)$ design matrix and $\textbf{Y}$ is a $n \times 1$ response vector, with independent entries.
Thus $\text{E}(\textbf{Y})=\text{X}\bf{\beta}$ and covariance matrix $\text{Cov}(\textbf{Y})=\sigma^2 \text{I}$

The first estimator $\hat{\beta}$ is the classical least squares estimator, defined as $$\bf{\hat{\beta}} = (\text{X}^T\text{X})^{-1}\text{X}^T\textbf{Y}$$.
**Q16:** 
$$\text{E}(\boldsymbol{\hat{\beta}}) = (\text{X}^T\text{X})^{-1}\text{X}^T\text{E}[\textbf{Y}] = (\text{X}^T\text{X})^{-1}\text{X}^T\text{X} \boldsymbol{\beta} = \boldsymbol{\beta}$$ 
$$\text{Var}(\boldsymbol{\hat{\beta}}) = (\text{X}^T\text{X})^{-1}\text{X}^T \sigma^2 \text{I} \big[ (\text{X}^T\text{X})^{-1}\text{X}^T\big]^T = \sigma^2 (\text{X}^T\text{X})^{-1}\text{X}^T\text{X}(\text{X}^T\text{X})^{-1} = \sigma^2 (\text{X}^T\text{X})^{-1}$$
**Q17:**
$\hat{f}(\mathbf{x_0}) = \bf{x_0}^T\boldsymbol{\hat{\beta}}$, thus:
$$\text{E}[\hat{f}(\mathbf{x_0})] = \mathbf{x_0}^T \text{E}[\boldsymbol{\hat{\beta}}] = \mathbf{x_0}^T \boldsymbol{\beta}$$
and
$$\text{Var}(\hat{f}(\mathbf{x_0})) = \mathbf{x_0}^T\sigma^2(\text{X}^T\text{X})^{-1}\mathbf{x_0}$$
**Q18:** 
$$\text{E}[(Y_0-\hat{f}(\mathbf{x_0}))]=[\text{E}(\hat{f}(\mathbf{x_0})-f(\mathbf{x}_0))]^2+\text{Var}(\hat{f}(\mathbf{x}_0) ) + \text{Var}(\boldsymbol{\epsilon}) = 0 + \sigma^2\mathbf{x_0}T(\text{X}^T\text{X})^{-1}\mathbf{x_0} + \sigma^2 \\
= \sigma^2(\mathbf{x_0}^T(\text{X}^T\text{X})^{-1}\mathbf{x_0} + I)$$

The second estimator of $\boldsymbol{\beta}$ is the Ridge regression estimator, defined as 

$$\widetilde{\boldsymbol \beta}=(\text{X}^T\text{X}+\lambda \text{I})^{-1}\text{X}^T{\mathbf Y}$$
Where the $\lambda$ is called a regularization parameter. Notice that for $\lambda = 0$, $\boldsymbol{\hat{\beta}}=\boldsymbol{\widetilde{\beta}}$. 
**Q19:** 
$$\text{E}[\boldsymbol{\widetilde{\beta}}] = (\text{X}^T\text{X} + \lambda I)^{-1}\text{X}^T \: \text{E}[\mathbf{Y}] = (\text{X}^T\text{X} + \lambda I)^{-1}\text{X}^T\text{X}\boldsymbol{\beta},$$ 
thus the estimator is biased.
$$\text{Var}(\boldsymbol{\widetilde{\beta}}) = (\text{X}^T\text{X} + \lambda I)^{-1}\text{X}^T \text{Var}(\mathbf{Y})\big[ (\text{X}^T\text{X} + \lambda I)^{-1}\text{X}^T \big]^T = (\text{X}^T\text{X} + \lambda I)^{-1}\text{X}^T \sigma^2 I \big[ (\text{X}^T\text{X} + \lambda I)^{-1}\text{X}^T \big]^T \\
= \sigma^2(\text{X}^T\text{X} + \lambda I)^{-1}\text{X}^T\big[ \text{X}(\text{X}^T\text{X}+\lambda I)^{-1} \big] = \sigma^2(\text{X}^T\text{X} + \lambda I)^{-1}\text{X}^T\text{X}(\text{X}^T\text{X}+\lambda I)^{-1}$$

**Q20:** 

**Q21:** 
$$\text{E}[(Y_0-\widetilde{f}({\bf x}_0))^2]=[\text{E}(\widetilde{f}({\bf x}_0)-f({\bf x}_0)]^2+\text{Var}(\widetilde{f}({\bf x}_0) ) + \text{Var}(\varepsilon)$$


```{r}
values=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/BVtradeoffvalues.dd")
X=values$X
dim(X)
x0=values$x0
dim(x0)
beta=values$beta
dim(beta)
sigma=values$sigma
sigma
```


**Q22:** 

```{r,eval=FALSE}
sqbias=function(lambda,X,x0,beta)
{
  p=dim(X)[2]
  value= #HERE YOU FILL IN
  return(value)
}
thislambda=seq(0,2,length=500)
sqbiaslambda=rep(NA,length(thislambda))
for (i in 1:length(thislambda)) sqbiaslambda[i]=sqbias(thislambda[i],X,x0,beta)
plot(thislambda,sqbiaslambda,col=2,type="l")
```

**Q23:** 

```{r,eval=FALSE}
variance=function(lambda,X,x0,sigma)
{
  p=dim(X)[2]
  inv=solve(t(X)%*%X+lambda*diag(p))
  value=#HERE YOU FILL IN
  return(value)
}
thislambda=seq(0,2,length=500)
variancelambda=rep(NA,length(thislambda))
for (i in 1:length(thislambda)) variancelambda[i]=variance(thislambda[i],X,x0,sigma)
plot(thislambda,variancelambda,col=4,type="l")
```


**Q24:** 

```{r,eval=FALSE}
tot=sqbiaslambda+variancelambda+sigma^2
which.min(tot)
thislambda[which.min(tot)]
plot(thislambda,tot,col=1,type="l",ylim=c(0,max(tot)))
lines(thislambda, sqbiaslambda,col=2)
lines(thislambda, variancelambda,col=4)
lines(thislambda,rep(sigma^2,500),col="orange")
abline(v=thislambda[which.min(tot)],col=3)
```

slkughsoliufbh+
