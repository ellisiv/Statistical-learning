\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Compulsory exercise 1},
            pdfauthor={Ellisiv Steen, Johanne Skogvang, Helene Behrens},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Compulsory exercise 1}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Ellisiv Steen, Johanne Skogvang, Helene Behrens}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{20 februar, 2019}


\begin{document}
\maketitle

\subsection{Problem 1: Multiple linear
regression}\label{problem-1-multiple-linear-regression}

\begin{verbatim}
## 
## Call:
## lm(formula = log(FEV) ~ Age + Htcm + Gender + Smoke, data = lungcap)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.63278 -0.08657  0.01146  0.09540  0.40701 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -1.943998   0.078639 -24.721  < 2e-16 ***
## Age          0.023387   0.003348   6.984  7.1e-12 ***
## Htcm         0.016849   0.000661  25.489  < 2e-16 ***
## GenderM      0.029319   0.011719   2.502   0.0126 *  
## Smoke       -0.046067   0.020910  -2.203   0.0279 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.1455 on 649 degrees of freedom
## Multiple R-squared:  0.8106, Adjusted R-squared:  0.8095 
## F-statistic: 694.6 on 4 and 649 DF,  p-value: < 2.2e-16
\end{verbatim}

\textbf{Q1:} The fitted expression for modelA is constructed from the
section ``Estimate'' in the summary. The expression is on the form

\[\textrm{log(FEV)} = \beta_0 + \beta_1 \cdot \textrm{Age} + \beta_2 \cdot \textrm{Htcm} + \beta_3 \cdot \textrm{GenderM} + \beta_4 \cdot \textrm{Smoke} \\
= -1.944 + 0.023\cdot \textrm{Age} + 0.017 \cdot \textrm{Htcm} + 0.029 \cdot \textrm{GenderM} - 0.046 \cdot \textrm{Smoke}\]

\textbf{Q2:} In the summary of modelA, there are some terms that need
explaining:

\begin{itemize}
\item
  Estimate: The estimated weights of the covariates in the fitted model.
  This is interpreted as the increase of the response variable as the
  covariate increases with 1 unit. Those are the \(\beta\)s used in Q1.
  The particular value ``Intercept'' is the value of the response
  variable when all covariates have value zero.
\item
  Std.Error: The std.Error or the standard deviation error is the error
  in the estimates for \(\beta\), that is:
  \(\sqrt{\textrm{Var}(\beta\textrm{s})}\)
\item
  Residual standard error:
  \(\sqrt{\frac{\textrm{RSS}}{\textrm{degrees of freedom}}}\) This is
  the standard deviation of the residuals. The residuals are a measure
  of the distance between the data points and the fitted model.
\item
  F-statistic: The F-statistic is a hypothesis test which compares our
  fitted model to a much simpler alternative model. This model is called
  a ``Intercept only model'' which is a linear model with \(\beta_0\) as
  only non-zero weight. The p-value of the F-statistic is the
  probability that the ``Intercept only model'' is as good as our fitted
  model, i.e.~the probability that all \(\beta\)s\(=0\) except for
  \(\beta_0\).
\end{itemize}

\textbf{Q3:} The proportion of variability explained by the fitted
modelA is represented my the ``Multiple R-squared'' and is in our case
approximately 81\%. This means that 81\% of the observed variance is
explained by our linear model, which is a fairly good score.

\textbf{Q4:} At first glance the plot below of the fitted values
vs.~residuals, appears normally distributed around 0. By further
inspection we see that the points are slightly shiftet upwards around
Fitted value = 0.8 and there seems to be a slightly higher variance for
very high and very low fitted values. It is also an important
observation that there seems to be more extreme negative residuals, but
there are more, and less extreme, positive residual observations. This
last observation is also supported by the Q-Q plot. Here we see a
curvature which may imply a slightly left skewed distribution. The
Anderson-Darling normality test also rejects the null hypothesis that
the data follows a normal distribution.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# residuls vs fitted}
\KeywordTok{ggplot}\NormalTok{(modelA, }\KeywordTok{aes}\NormalTok{(.fitted, .stdresid)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{pch =} \DecValTok{21}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \DecValTok{0}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{method =} \StringTok{"loess"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Fitted values"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Standardized residuals"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Fitted values vs. standardized residuals for model A"}\NormalTok{,}
       \DataTypeTok{subtitle =} \KeywordTok{deparse}\NormalTok{(modelA}\OperatorTok{$}\NormalTok{call))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rmarkdown1_files/figure-latex/unnamed-chunk-1-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# qq-plot of residuals}
\KeywordTok{ggplot}\NormalTok{(modelA, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =}\NormalTok{ .stdresid)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_qq}\NormalTok{(}\DataTypeTok{pch =} \DecValTok{19}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{intercept =} \DecValTok{0}\NormalTok{, }\DataTypeTok{slope =} \DecValTok{1}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dotted"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Theoretical quantiles"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Standardized residuals"}\NormalTok{, }
       \DataTypeTok{title =} \StringTok{"Normal Q-Q"}\NormalTok{, }\DataTypeTok{subtitle =} \KeywordTok{deparse}\NormalTok{(modelA}\OperatorTok{$}\NormalTok{call))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rmarkdown1_files/figure-latex/unnamed-chunk-1-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# normality test}
\KeywordTok{library}\NormalTok{(nortest) }
\KeywordTok{ad.test}\NormalTok{(}\KeywordTok{rstudent}\NormalTok{(modelA))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Anderson-Darling normality test
## 
## data:  rstudent(modelA)
## A = 1.9256, p-value = 6.486e-05
\end{verbatim}

\textbf{Q5:} We now introduce a new model, modelB, which is defined
below. We see that in modelB, the residuals are not equally distributed
around zero and the variance increases for higher fitted values. We
would prefere to use modelA when making inference about FEV because the
mean value of the residuals in modelA is approximately zero for all
fitted values and the variance is stabile. We have observed that none of
this is true for modelB.

\includegraphics{Rmarkdown1_files/figure-latex/unnamed-chunk-2-1.pdf}

\textbf{Q6:} We are interessted in finding out if the \(\beta_1 = 0\),
which means that all observed correlation between Age and FEV are due to
chance. The resulting p-value of the hypothesis test is presented in the
summary as the covariate's
\(\text{Pr(>|t|)}_{\text{Age}} = 7.1\cdot 10^{-12}\). This is a very
small p-value, it is interpreted as the probability that we observe this
correlation given \(\beta_1 = 0\) is \(7.1 \cdot 10^{-12}\). Considering
that we would reject \(\text{H}_0\) at \(p < 0.05\), we definately
reject it in our case.

\textbf{Q7:} A prediction interval for a t-distributed variable is on
the form
\[\Bigg[ \hat{\beta_1} \pm t_{\frac{\alpha}{2}, n-p-1}\text{SD}(\hat{\beta_1}) \Bigg] = \Bigg[ \hat{\beta_1} \pm t_{0.005,654-4-1}\text{SD}(\hat{\beta_1}) \Bigg] \]\\
This is for our model given as:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cat}\NormalTok{(}\StringTok{'['}\NormalTok{,}\KeywordTok{confint}\NormalTok{(modelA, }\DataTypeTok{level=}\FloatTok{0.99}\NormalTok{)[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{],}\StringTok{', '}\NormalTok{,}\KeywordTok{confint}\NormalTok{(modelA, }\DataTypeTok{level=}\FloatTok{0.99}\NormalTok{)[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{],}\StringTok{']'}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [ 0.01473674 ,  0.03203769 ]
\end{verbatim}

This tells us that there is a 99\% chance of that the true value of
\(\beta_1\) lies between {[}0.0147, 0.0320{]}.

\textbf{Q8:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Age=}\DecValTok{16}\NormalTok{, }\DataTypeTok{Htcm=}\DecValTok{170}\NormalTok{, }\DataTypeTok{Gender=}\StringTok{"M"}\NormalTok{, }\DataTypeTok{Smoke=}\DecValTok{0}\NormalTok{)}

\NormalTok{prelog <-}\StringTok{ }\KeywordTok{predict.lm}\NormalTok{(modelA, new, }\DataTypeTok{interval =} \StringTok{"prediction"}\NormalTok{, }\DataTypeTok{level =} \FloatTok{0.95}\NormalTok{)}

\NormalTok{prediction <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(prelog[}\DecValTok{1}\NormalTok{])}
\NormalTok{low <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(prelog[}\DecValTok{2}\NormalTok{])}
\NormalTok{upp <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(prelog[}\DecValTok{3}\NormalTok{])}

\KeywordTok{cat}\NormalTok{(}\StringTok{'Predicted "forced expiratory colume" (FEV) = '}\NormalTok{, prediction, }\StringTok{'. }\CharTok{\textbackslash{}n}\StringTok{ 95% confidence intercal = ['}\NormalTok{,low, }\StringTok{','}\NormalTok{,upp,}\StringTok{']'}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Predicted "forced expiratory colume" (FEV) =  3.75768 . 
##  95% confidence intercal = [ 2.818373 , 5.010038 ]
\end{verbatim}

A little reasearch shows that normal FEV for males lies between
approximately 2 and 5, which means that the information provided by the
prediction interval is of little value as the interval is relatively
wide.

\section{Problem 2: Classification}\label{problem-2-classification}

\textbf{Q9:} The mathematical formula for the K-nearest neighbours
estimator is
\(\hat{y}(x) = \underset{j}{\mathrm{argmax}} \hat{P}(Y = j|X = x_0)\),
where
\[\hat{P}(Y = j | X = x_0) = \frac{1}{K} \sum_{i \in \mathcal{N}_0} I(y_i = j)\]
and \(j \in \{0,\: 1\}\)

\textbf{Q10:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k_values =}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{30}

\NormalTok{test.e <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\DataTypeTok{mode =} \StringTok{"numeric"}\NormalTok{, }\DataTypeTok{length =} \DecValTok{30}\NormalTok{)}
\NormalTok{train.e <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\DataTypeTok{mode =} \StringTok{"numeric"}\NormalTok{, }\DataTypeTok{length =} \DecValTok{30}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{(k }\ControlFlowTok{in}\NormalTok{ k_values)\{}
\NormalTok{  test.e[k] =}\StringTok{ }\KeywordTok{mean}\NormalTok{(Mdf[}\OperatorTok{-}\NormalTok{tr,]}\OperatorTok{$}\NormalTok{y }\OperatorTok{!=}\StringTok{ }\KeywordTok{knn}\NormalTok{(Mdf[tr,], Mdf[}\OperatorTok{-}\NormalTok{tr,],Mdf[tr,]}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{k =}\NormalTok{ k, }\DataTypeTok{l =} \DecValTok{0}\NormalTok{))}
\NormalTok{  train.e[k] =}\StringTok{ }\KeywordTok{mean}\NormalTok{(Mdf[tr,]}\OperatorTok{$}\NormalTok{y }\OperatorTok{!=}\StringTok{ }\KeywordTok{knn}\NormalTok{(Mdf[tr,], Mdf[tr,],Mdf[tr,]}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{k =}\NormalTok{ k, }\DataTypeTok{l =} \DecValTok{0}\NormalTok{))}
\NormalTok{\}}
\KeywordTok{qplot}\NormalTok{(k_values, test.e)}\OperatorTok{+}\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title=}\StringTok{"Misclassification error for test data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rmarkdown1_files/figure-latex/unnamed-chunk-6-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(k_values, train.e)}\OperatorTok{+}\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title=}\StringTok{"Misclassification error for training data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rmarkdown1_files/figure-latex/unnamed-chunk-6-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{ks =}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{30} \CommentTok{# Choose K from 1 to 30.}
\NormalTok{idx =}\StringTok{ }\KeywordTok{createFolds}\NormalTok{(M[tr,}\DecValTok{1}\NormalTok{], }\DataTypeTok{k=}\DecValTok{5}\NormalTok{) }\CommentTok{# Divide the training data into 5 folds.}
\CommentTok{# "Sapply" is a more efficient for-loop. }
\CommentTok{# We loop over each fold and each value in "ks"}
\CommentTok{# and compute error rates for each combination.}
\CommentTok{# All the error rates are stored in the matrix "cv", }
\CommentTok{# where folds are rows and values of $K$ are columns.}
\NormalTok{cv =}\StringTok{ }\KeywordTok{sapply}\NormalTok{(ks, }\ControlFlowTok{function}\NormalTok{(k)\{ }
  \KeywordTok{sapply}\NormalTok{(}\KeywordTok{seq_along}\NormalTok{(idx), }\ControlFlowTok{function}\NormalTok{(j) \{}
\NormalTok{    yhat =}\StringTok{ }\NormalTok{class}\OperatorTok{::}\KeywordTok{knn}\NormalTok{(}\DataTypeTok{train=}\NormalTok{M[tr[ }\OperatorTok{-}\NormalTok{idx[[j]] ], }\OperatorTok{-}\DecValTok{1}\NormalTok{],}
               \DataTypeTok{cl=}\NormalTok{M[tr[ }\OperatorTok{-}\NormalTok{idx[[j]] ], }\DecValTok{1}\NormalTok{],}
               \DataTypeTok{test=}\NormalTok{M[tr[ idx[[j]] ], }\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DataTypeTok{k =}\NormalTok{ k)}
    \KeywordTok{mean}\NormalTok{(M[tr[ idx[[j]] ], }\DecValTok{1}\NormalTok{] }\OperatorTok{!=}\StringTok{ }\NormalTok{yhat)}
\NormalTok{  \})}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\textbf{Q11:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.e <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\DataTypeTok{mode =} \StringTok{"numeric"}\NormalTok{, }\DataTypeTok{length =} \DecValTok{30}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{30}\NormalTok{)\{}
\NormalTok{  cv.e[i] =}\StringTok{ }\KeywordTok{mean}\NormalTok{(cv[,i])}
\NormalTok{\}}
\NormalTok{cv.se <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\DataTypeTok{mode =} \StringTok{"numeric"}\NormalTok{, }\DataTypeTok{length =} \DecValTok{30}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{30}\NormalTok{)\{}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)\{}
\NormalTok{    cv.se[i] =}\StringTok{ }\NormalTok{cv.se[i] }\OperatorTok{+}\StringTok{ }\NormalTok{(cv[j,i]}\OperatorTok{-}\StringTok{ }\NormalTok{cv.e[i]) }\OperatorTok{**}\StringTok{ }\DecValTok{2}
\NormalTok{  \}}
\NormalTok{  cv.se[i] =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{5} \OperatorTok{*}\StringTok{ }\NormalTok{cv.se[i])}
\NormalTok{\}}

\NormalTok{k.min <-}\StringTok{ }\KeywordTok{which.min}\NormalTok{(cv.e)}
\end{Highlighting}
\end{Shaded}

\textbf{Q12:} The bias in a KNN method will increase with an increasing
K and this is due to decreasing complexity. A very complex model can fit
models which is less or equal in complexity, and that means that when
the complexity decreases we are not able to fit complex ``true models''.
This explains why the bias increases for increasing K in general. The
variance on the other hand, decreases for increasing K because the
estimated models becomes more stable for perturbations in the data set.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(colorspace)}
\NormalTok{co =}\StringTok{ }\KeywordTok{rainbow_hcl}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}\OperatorTok{+}\NormalTok{.}\DecValTok{1}\NormalTok{, }\DataTypeTok{mgp =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(ks, cv.e, }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{16}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.7}\NormalTok{), }\DataTypeTok{col =}\NormalTok{ co[}\DecValTok{2}\NormalTok{],}
     \DataTypeTok{xlab =} \StringTok{"Number of neighbors"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Misclassification error"}\NormalTok{)}
\KeywordTok{arrows}\NormalTok{(ks, cv.e}\OperatorTok{-}\NormalTok{cv.se, ks, cv.e}\OperatorTok{+}\NormalTok{cv.se, }\DataTypeTok{angle=}\DecValTok{90}\NormalTok{, }\DataTypeTok{length=}\NormalTok{.}\DecValTok{03}\NormalTok{, }\DataTypeTok{code=}\DecValTok{3}\NormalTok{, }\DataTypeTok{col=}\NormalTok{co[}\DecValTok{2}\NormalTok{])}
\KeywordTok{lines}\NormalTok{(ks, train.e, }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{16}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.7}\NormalTok{), }\DataTypeTok{col =}\NormalTok{ co[}\DecValTok{3}\NormalTok{])}
\KeywordTok{lines}\NormalTok{(ks, test.e, }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{16}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.7}\NormalTok{), }\DataTypeTok{col =}\NormalTok{ co[}\DecValTok{1}\NormalTok{])}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"Test"}\NormalTok{, }\StringTok{"5-fold CV"}\NormalTok{, }\StringTok{"Training"}\NormalTok{), }\DataTypeTok{lty =} \DecValTok{1}\NormalTok{, }\DataTypeTok{col=}\NormalTok{co)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rmarkdown1_files/figure-latex/unnamed-chunk-9-1.pdf}

\textbf{Q13:} The most important properties of a good value for K, is
that it provides a small misclassification error and it should produce a
prediction model with low complexity. The strategy is to find the
k-value which produces the smallest misclassification error in the
5-fold test and see if we can choose a higher k with not that much worse
misclassification error. We accept a misclassification error equal to
the error of the best k plus one time its standard deviation, and choose
the largest k satisfying this condition.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k =}\StringTok{ }\KeywordTok{tail}\NormalTok{(}\KeywordTok{which}\NormalTok{(cv.e }\OperatorTok{<}\StringTok{ }\NormalTok{cv.e[k.min] }\OperatorTok{+}\StringTok{ }\NormalTok{cv.se[k.min]), }\DecValTok{1}\NormalTok{)}
\NormalTok{size =}\StringTok{ }\DecValTok{100}
\NormalTok{xnew =}\StringTok{ }\KeywordTok{apply}\NormalTok{(M[tr,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(X) }\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(X), }\KeywordTok{max}\NormalTok{(X), }\DataTypeTok{length.out=}\NormalTok{size))}
\NormalTok{grid =}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(xnew[,}\DecValTok{1}\NormalTok{], xnew[,}\DecValTok{2}\NormalTok{])}
\NormalTok{grid.yhat =}\StringTok{ }\KeywordTok{knn}\NormalTok{(M[tr,}\OperatorTok{-}\DecValTok{1}\NormalTok{], M[tr,}\DecValTok{1}\NormalTok{], }\DataTypeTok{k=}\NormalTok{k, }\DataTypeTok{test=}\NormalTok{grid)}
\NormalTok{np =}\StringTok{ }\DecValTok{300}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{), }\DataTypeTok{mgp =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\KeywordTok{contour}\NormalTok{(xnew[,}\DecValTok{1}\NormalTok{], xnew[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{z =} \KeywordTok{matrix}\NormalTok{(grid.yhat, size), }\DataTypeTok{levels=}\NormalTok{.}\DecValTok{5}\NormalTok{, }
        \DataTypeTok{xlab=}\KeywordTok{expression}\NormalTok{(}\StringTok{"x"}\NormalTok{[}\DecValTok{1}\NormalTok{]), }\DataTypeTok{ylab=}\KeywordTok{expression}\NormalTok{(}\StringTok{"x"}\NormalTok{[}\DecValTok{2}\NormalTok{]), }\DataTypeTok{axes=}\OtherTok{FALSE}\NormalTok{,}
        \DataTypeTok{main =} \KeywordTok{paste0}\NormalTok{(k,}\StringTok{"-nearest neighbors"}\NormalTok{), }\DataTypeTok{cex=}\FloatTok{1.2}\NormalTok{, }\DataTypeTok{labels=}\StringTok{""}\NormalTok{)}
\KeywordTok{points}\NormalTok{(grid, }\DataTypeTok{pch=}\StringTok{"."}\NormalTok{, }\DataTypeTok{cex=}\DecValTok{1}\NormalTok{, }\DataTypeTok{col=}\NormalTok{grid.yhat)}
\KeywordTok{points}\NormalTok{(M[}\DecValTok{1}\OperatorTok{:}\NormalTok{np,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DataTypeTok{col=}\KeywordTok{factor}\NormalTok{(M[}\DecValTok{1}\OperatorTok{:}\NormalTok{np,}\DecValTok{1}\NormalTok{]), }\DataTypeTok{pch =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lwd =} \FloatTok{1.5}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"Player 1 wins"}\NormalTok{, }\StringTok{"Player 2 wins"}\NormalTok{), }
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"black"}\NormalTok{), }\DataTypeTok{pch=}\DecValTok{1}\NormalTok{)}
\KeywordTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rmarkdown1_files/figure-latex/unnamed-chunk-10-1.pdf}

\textbf{Q14:}

We plot an ROC curve, an Receiver Operating Characteristics curve, which
compares the specificity and the sensitivity of the classifier. We
define the specificity and sensitivity as
\[\text{Specificity} = \frac{\text{True Negative}}{\text{Negiative}}\\ \text{Sensitivity} = \frac{\text{True Positive}}{\text{Positive}}.\]
An ideal model would have both specificity and sensitivity of 1, which
would mean that all wins (positives) would be classified as wins, and
all losses (negatives) would be classified as losses. We plot the
sensitivity against the specificity for different threshold rates, with
the sensitivity increasing from zero and the specificity decreasing from
one, and we will thus wish for the curve to hug the upper left corner of
the plot. We can think of the threshold as the porbability p,for which
we classify the match as won;

The match is classified as won by player 1 if P(Player 1 wins the game)
\textgreater{} p.

When we do random guessing, we will expect that the number of wins that
are correctly classified increases with a decreasing p, as it means that
over all, more matches will be classified as wins. Equivalently, the
number of correctly classified losses will decrease. Thus, when p
decreases from 1 to 0, the relation between the spesificity and the
sensitivity will change accordingly, and we will observe a straight
line; \[\text{Sensitivity} = 1 - \text{Specificity}\] This will also
mean that the area under the curve, the AUC, is 0.5.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{K=}\DecValTok{30}
  
\CommentTok{# knn with prob=TRUE outputs the probability of the winning class}
\CommentTok{# therefore we have to do an extra step to get the probability of player 1 winning}
\NormalTok{KNNclass=class}\OperatorTok{::}\KeywordTok{knn}\NormalTok{(}\DataTypeTok{train=}\NormalTok{M[tr,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DataTypeTok{cl=}\NormalTok{M[tr,}\DecValTok{1}\NormalTok{], }\DataTypeTok{test=}\NormalTok{M[}\OperatorTok{-}\NormalTok{tr,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DataTypeTok{k =}\NormalTok{ K,}\DataTypeTok{prob=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{KNNprobwinning=}\KeywordTok{attributes}\NormalTok{(KNNclass)}\OperatorTok{$}\NormalTok{prob}
\NormalTok{KNNprob=}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(KNNclass }\OperatorTok{==}\StringTok{ "0"}\NormalTok{, }\DecValTok{1}\OperatorTok{-}\NormalTok{KNNprobwinning, KNNprobwinning)}
\CommentTok{# now KNNprob has probability that player 1 wins, for all matches in the test set}

\KeywordTok{library}\NormalTok{(pROC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Type 'citation("pROC")' for a citation.
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'pROC'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:colorspace':
## 
##     coords
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:stats':
## 
##     cov, smooth, var
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# now you use predictor=KNNprob and response=M[-tr,1] }
\CommentTok{# in your call to the function roc in the pROC library-tr,1}
\CommentTok{#p}
\NormalTok{M_roc =}\StringTok{ }\KeywordTok{roc}\NormalTok{(M[}\OperatorTok{-}\NormalTok{tr,}\DecValTok{1}\NormalTok{], KNNprob, }\DataTypeTok{lagacy.axes =} \OtherTok{TRUE}\NormalTok{ )}
\NormalTok{M_auc <-}\StringTok{ }\KeywordTok{auc}\NormalTok{(M_roc)}
\NormalTok{M_auc}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Area under the curve: 0.8178
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggroc}\NormalTok{(M_roc) }\OperatorTok{+}\StringTok{ }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"ROC"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Specificity"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Sensitivity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rmarkdown1_files/figure-latex/unnamed-chunk-11-1.pdf}

\textbf{Q15:}

We see that the misclassification error on the test data is lower with
\(\tilde{y}\) than with \(\hat{y}\), and thus we would prefer to use
\(\tilde{y}\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#The code from Q13:}
\KeywordTok{library}\NormalTok{(}\StringTok{"caret"}\NormalTok{)}

\NormalTok{k =}\StringTok{ }\KeywordTok{tail}\NormalTok{(}\KeywordTok{which}\NormalTok{(cv.e }\OperatorTok{<}\StringTok{ }\NormalTok{cv.e[k.min] }\OperatorTok{+}\StringTok{ }\NormalTok{cv.se[k.min]), }\DecValTok{1}\NormalTok{)}
\NormalTok{size =}\StringTok{ }\DecValTok{100}
\NormalTok{xnew =}\StringTok{ }\KeywordTok{apply}\NormalTok{(M[tr,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(X) }\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(X), }\KeywordTok{max}\NormalTok{(X), }\DataTypeTok{length.out=}\NormalTok{size))}
\NormalTok{grid =}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(xnew[,}\DecValTok{1}\NormalTok{], xnew[,}\DecValTok{2}\NormalTok{])}
\NormalTok{grid.yhat =}\StringTok{ }\KeywordTok{knn}\NormalTok{(M[tr,}\OperatorTok{-}\DecValTok{1}\NormalTok{], M[tr,}\DecValTok{1}\NormalTok{], }\DataTypeTok{k=}\NormalTok{k, }\DataTypeTok{test=}\NormalTok{grid)}
\NormalTok{np =}\StringTok{ }\DecValTok{300}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{), }\DataTypeTok{mgp =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\KeywordTok{contour}\NormalTok{(xnew[,}\DecValTok{1}\NormalTok{], xnew[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{z =} \KeywordTok{matrix}\NormalTok{(grid.yhat, size), }\DataTypeTok{levels=}\NormalTok{.}\DecValTok{5}\NormalTok{, }
        \DataTypeTok{xlab=}\KeywordTok{expression}\NormalTok{(}\StringTok{"x"}\NormalTok{[}\DecValTok{1}\NormalTok{]), }\DataTypeTok{ylab=}\KeywordTok{expression}\NormalTok{(}\StringTok{"x"}\NormalTok{[}\DecValTok{2}\NormalTok{]), }\DataTypeTok{axes=}\OtherTok{FALSE}\NormalTok{,}
        \DataTypeTok{main =} \KeywordTok{paste0}\NormalTok{(k,}\StringTok{"-nearest neighbors"}\NormalTok{), }\DataTypeTok{cex=}\FloatTok{1.2}\NormalTok{, }\DataTypeTok{labels=}\StringTok{""}\NormalTok{)}
\KeywordTok{points}\NormalTok{(grid, }\DataTypeTok{pch=}\StringTok{"."}\NormalTok{, }\DataTypeTok{cex=}\DecValTok{1}\NormalTok{, }\DataTypeTok{col=}\NormalTok{grid.yhat)}
\KeywordTok{points}\NormalTok{(M[}\DecValTok{1}\OperatorTok{:}\NormalTok{np,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DataTypeTok{col=}\KeywordTok{factor}\NormalTok{(M[}\DecValTok{1}\OperatorTok{:}\NormalTok{np,}\DecValTok{1}\NormalTok{]), }\DataTypeTok{pch =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lwd =} \FloatTok{1.5}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{)}

\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"Player 1 wins"}\NormalTok{, }\StringTok{"Player 2 wins"}\NormalTok{), }
       \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"black"}\NormalTok{), }\DataTypeTok{pch=}\DecValTok{1}\NormalTok{)}
\KeywordTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rmarkdown1_files/figure-latex/unnamed-chunk-12-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#print(Mdf)}
\CommentTok{#print(Mdf[-tr,1])}


\CommentTok{#Confusion matrix for test data:}
\NormalTok{conf_knn <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{knn}\NormalTok{(Mdf[tr,],Mdf[}\OperatorTok{-}\NormalTok{tr,],Mdf[tr,]}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{k =} \DecValTok{30}\NormalTok{, }\DataTypeTok{l =} \DecValTok{0}\NormalTok{),Mdf[}\OperatorTok{-}\NormalTok{tr,]}\OperatorTok{$}\NormalTok{y)}
\KeywordTok{print}\NormalTok{(conf_knn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 141  41
##          1  53 159
##                                           
##                Accuracy : 0.7614          
##                  95% CI : (0.7162, 0.8027)
##     No Information Rate : 0.5076          
##     P-Value [Acc > NIR] : <2e-16          
##                                           
##                   Kappa : 0.5223          
##  Mcnemar's Test P-Value : 0.2566          
##                                           
##             Sensitivity : 0.7268          
##             Specificity : 0.7950          
##          Pos Pred Value : 0.7747          
##          Neg Pred Value : 0.7500          
##              Prevalence : 0.4924          
##          Detection Rate : 0.3579          
##    Detection Prevalence : 0.4619          
##       Balanced Accuracy : 0.7609          
##                                           
##        'Positive' Class : 0               
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cat}\NormalTok{(}\StringTok{"Misclassification error for knn: "}\NormalTok{, }\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{conf_knn}\OperatorTok{$}\NormalTok{overall[}\StringTok{'Accuracy'}\NormalTok{],}\StringTok{'}\CharTok{\textbackslash{}n}\StringTok{'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Misclassification error for knn:  0.2385787
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cat}\NormalTok{(}\StringTok{"Confusion matrix for the argmax classifier }\CharTok{\textbackslash{}n}\StringTok{: }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion matrix for the argmax classifier 
## :
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test.y <-}\StringTok{ }\NormalTok{M[}\OperatorTok{-}\NormalTok{tr,}\DecValTok{1}\NormalTok{]}
\NormalTok{test.x1 <-}\StringTok{ }\NormalTok{M[}\OperatorTok{-}\NormalTok{tr,}\DecValTok{2}\NormalTok{]}
\NormalTok{test.x2 <-}\StringTok{ }\NormalTok{M[}\OperatorTok{-}\NormalTok{tr,}\DecValTok{3}\NormalTok{]}

\NormalTok{int_x1 <-}\StringTok{ }\KeywordTok{strtoi}\NormalTok{(test.x1)}
\NormalTok{int_x2 <-}\StringTok{ }\KeywordTok{strtoi}\NormalTok{(test.x2)}

\NormalTok{pred_argmax <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\DataTypeTok{mode =} \StringTok{"character"}\NormalTok{, }\DataTypeTok{length =} \KeywordTok{length}\NormalTok{(test.x1))}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(int_x1))\{}
  \ControlFlowTok{if}\NormalTok{(int_x1[i] }\OperatorTok{>=}\StringTok{ }\NormalTok{int_x2[i])\{}
\NormalTok{    pred_argmax[i] =}\StringTok{ "1"}
\NormalTok{    \}}
  \ControlFlowTok{else}\NormalTok{\{}
\NormalTok{    pred_argmax[i] =}\StringTok{ "0"}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{#lager v√•r egen confusion matrix:}
\NormalTok{TN <-}\StringTok{ }\DecValTok{0}
\NormalTok{FP <-}\StringTok{ }\DecValTok{0}
\NormalTok{FN <-}\StringTok{ }\DecValTok{0}
\NormalTok{TP <-}\StringTok{ }\DecValTok{0}

\ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(test.x1))\{}
  \ControlFlowTok{if}\NormalTok{(test.y[j] }\OperatorTok{==}\StringTok{ "1"} \OperatorTok{&&}\StringTok{ }\NormalTok{pred_argmax[j] }\OperatorTok{==}\StringTok{ "1"}\NormalTok{)\{}
\NormalTok{    TP =}\StringTok{ }\NormalTok{TP }\OperatorTok{+}\DecValTok{1}
\NormalTok{  \}}
  \ControlFlowTok{if}\NormalTok{(test.y[j] }\OperatorTok{==}\StringTok{ "1"} \OperatorTok{&&}\StringTok{ }\NormalTok{pred_argmax[j] }\OperatorTok{==}\StringTok{ "0"}\NormalTok{)\{}
\NormalTok{    FN =}\StringTok{ }\NormalTok{FN }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{  \}}
  \ControlFlowTok{if}\NormalTok{(test.y[j] }\OperatorTok{==}\StringTok{ "0"} \OperatorTok{&&}\StringTok{ }\NormalTok{pred_argmax[j] }\OperatorTok{==}\StringTok{ "0"}\NormalTok{)\{}
\NormalTok{    TN =}\StringTok{ }\NormalTok{TN }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{  \}}
  \ControlFlowTok{if}\NormalTok{(test.y[j] }\OperatorTok{==}\StringTok{ "0"} \OperatorTok{&&}\StringTok{ }\NormalTok{pred_argmax[j] }\OperatorTok{==}\StringTok{ "1"}\NormalTok{)\{}
\NormalTok{    FP =}\StringTok{ }\NormalTok{FP }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{  \}}
\NormalTok{\}}

\NormalTok{argmax_cm <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(TN,FP,FN,TP), }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{, }\DataTypeTok{dimnames =} \KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)))}
\NormalTok{argmax_cm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     0   1
## 0 145  43
## 1  49 157
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cat}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{ Misclassification error for argmax classifier: "}\NormalTok{,(FP}\OperatorTok{+}\NormalTok{FN)}\OperatorTok{/}\NormalTok{(TN}\OperatorTok{+}\NormalTok{FP}\OperatorTok{+}\NormalTok{FN}\OperatorTok{+}\NormalTok{TP))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Misclassification error for argmax classifier:  0.2335025
\end{verbatim}

\section{Problem 3: Bias-variance
trade-off}\label{problem-3-bias-variance-trade-off}

Let \(\bf{x}\) be a \((p+1) \times 1\) vector of covariates (including a
constant 1 as the first term). We are considering a regression problem
\(Y = f(\bf{x})\), where \(\text{E}(\bf{\epsilon}) = 0\) and
\(\text{Var}(\bf{\epsilon}) = \sigma^2\). We assume that the true
function is really a linear combination of the observed covariates,
which means that the irreducible error only lies in the
\(\bf{\epsilon}\)-term.

We will consider two estimators for \(\bf{\beta}\). The estimators are
based on a training set where \(\text{X}\) is a \(n\times (p-1)\) design
matrix and \(\textbf{Y}\) is a \(n \times 1\) response vector, with
independent entries. Thus \(\text{E}(\textbf{Y})=\text{X}\bf{\beta}\)
and covariance matrix \(\text{Cov}(\textbf{Y})=\sigma^2 \text{I}\)

The first estimator \(\hat{\beta}\) is the classical least squares
estimator, defined as
\[\bf{\hat{\beta}} = (\text{X}^T\text{X})^{-1}\text{X}^T\textbf{Y}\].
\textbf{Q16:}
\[\text{E}(\boldsymbol{\hat{\beta}}) = (\text{X}^T\text{X})^{-1}\text{X}^T\text{E}[\textbf{Y}] = (\text{X}^T\text{X})^{-1}\text{X}^T\text{X} \boldsymbol{\beta} = \boldsymbol{\beta}\]
\[\text{Var}(\boldsymbol{\hat{\beta}}) = (\text{X}^T\text{X})^{-1}\text{X}^T \sigma^2 \text{I} \big[ (\text{X}^T\text{X})^{-1}\text{X}^T\big]^T = \sigma^2 (\text{X}^T\text{X})^{-1}\text{X}^T\text{X}(\text{X}^T\text{X})^{-1} = \sigma^2 (\text{X}^T\text{X})^{-1}\]
\textbf{Q17:}
\(\hat{f}(\mathbf{x_0}) = \bf{x_0}^T\boldsymbol{\hat{\beta}}\), thus:
\[\text{E}[\hat{f}(\mathbf{x_0})] = \mathbf{x_0}^T \text{E}[\boldsymbol{\hat{\beta}}] = \mathbf{x_0}^T \boldsymbol{\beta}\]
and
\[\text{Var}(\hat{f}(\mathbf{x_0})) = \mathbf{x_0}^T\sigma^2(\text{X}^T\text{X})^{-1}\mathbf{x_0}\]
\textbf{Q18:}
\[\text{E}[(Y_0-\hat{f}(\mathbf{x_0}))]=[\text{E}(\hat{f}(\mathbf{x_0})-f(\mathbf{x}_0))]^2+\text{Var}(\hat{f}(\mathbf{x}_0) ) + \text{Var}(\boldsymbol{\epsilon}) = 0 + \sigma^2\mathbf{x_0}T(\text{X}^T\text{X})^{-1}\mathbf{x_0} + \sigma^2 \\
= \sigma^2(\mathbf{x_0}^T(\text{X}^T\text{X})^{-1}\mathbf{x_0} + 1)\]

The second estimator of \(\boldsymbol{\beta}\) is the Ridge regression
estimator, defined as

\[\widetilde{\boldsymbol \beta}=(\text{X}^T\text{X}+\lambda \text{I})^{-1}\text{X}^T{\mathbf Y}\]
Where the \(\lambda\) is called a regularization parameter. Notice that
for \(\lambda = 0\),
\(\boldsymbol{\hat{\beta}}=\boldsymbol{\widetilde{\beta}}\). To
simplify, we define
\(\text{W} = (\text{X}^T\text{X}+\lambda \text{I})^{-1}\text{X}^T\).\\
\textbf{Q19:}
\[\text{E}[\boldsymbol{\widetilde{\beta}}] = (\text{X}^T\text{X} + \lambda I)^{-1}\text{X}^T \, \text{E}[\mathbf{Y}] = \text{W}\text{X}\boldsymbol{\beta},\]\\
thus the estimator is biased.\\
\[\text{Var}(\boldsymbol{\widetilde{\beta}}) = \text{W} \text{Var}(\mathbf{Y})\text{W}^T = \text{W} \sigma^2 I \text{W}^T 
= \sigma \text{W}\text{W}^T \sigma = \sigma(\text{X}^T\text{X} + \lambda I)^{-1}\text{X}^T\text{X}(\text{X}^T\text{X}+\lambda I)^{-1}\sigma\]

\textbf{Q20:}
\[\text{E}(\widetilde{f}(x_0)) = \text{E}(x_0^T \widetilde{\beta}) = x_0^T\text{E}(\widetilde{\beta})=x_0^T\text{W}\text{X}\beta\]

\[\text{Var}(\widetilde{f}(x_0)) = x_0^T\text{Var}(\widetilde{\beta})x_0 = \sigma x_0^T \text{W} \text{W}^T x_0 \sigma = \sigma (x_0^T\text{W})(x_0^T\text{W})^T\sigma\]

\textbf{Q21:}
\[\text{E}[(Y_0-\widetilde{f}({\bf x}_0))^2]=[\text{E}(\widetilde{f}({\bf x}_0)-f({\bf x}_0)]^2+\text{Var}(\widetilde{f}({\bf x}_0) ) + \text{Var}(\varepsilon) = (x_0^T\text{W}\text{X}\beta - x_0^T\beta)^2 + \sigma (x_0^T\text{W})(x_0^T\text{W})^T\sigma + \sigma^2\]

\begin{verbatim}
## [1] 100  81
\end{verbatim}

\begin{verbatim}
## [1] 81  1
\end{verbatim}

\begin{verbatim}
## [1] 81  1
\end{verbatim}

\begin{verbatim}
## [1] 0.5
\end{verbatim}

\textbf{Q22:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sqbias=}\ControlFlowTok{function}\NormalTok{(lambda,X,x0,beta)}
\NormalTok{\{}
\NormalTok{  p=}\KeywordTok{dim}\NormalTok{(X)[}\DecValTok{2}\NormalTok{]}
\NormalTok{  inv=}\KeywordTok{solve}\NormalTok{(}\KeywordTok{t}\NormalTok{(X)}\OperatorTok{%*%}\NormalTok{X}\OperatorTok{+}\NormalTok{lambda}\OperatorTok{*}\KeywordTok{diag}\NormalTok{(p))}
\NormalTok{  W =}\StringTok{ }\NormalTok{inv}\OperatorTok{%*%}\KeywordTok{t}\NormalTok{(X)}
\NormalTok{  value=}\StringTok{ }\NormalTok{(}\KeywordTok{t}\NormalTok{(x0)}\OperatorTok{%*%}\NormalTok{W}\OperatorTok{%*%}\NormalTok{X}\OperatorTok{%*%}\NormalTok{beta}\OperatorTok{-}\KeywordTok{t}\NormalTok{(x0)}\OperatorTok{%*%}\NormalTok{beta)}\OperatorTok{^}\DecValTok{2}
  \KeywordTok{return}\NormalTok{(value)}
\NormalTok{\}}
\NormalTok{thislambda=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DataTypeTok{length=}\DecValTok{500}\NormalTok{)}
\NormalTok{sqbiaslambda=}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\KeywordTok{length}\NormalTok{(thislambda))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(thislambda)) sqbiaslambda[i]=}\KeywordTok{sqbias}\NormalTok{(thislambda[i],X,x0,beta)}
\KeywordTok{plot}\NormalTok{(thislambda,sqbiaslambda,}\DataTypeTok{col=}\DecValTok{2}\NormalTok{,}\DataTypeTok{type=}\StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rmarkdown1_files/figure-latex/unnamed-chunk-14-1.pdf}

This does not exactly look as we expected, the squared bias is
increasing as we thought, but the dip around \(\lambda = 0.5\) was
unexpected. The expected increase in bias is due to a damping of the
estimated coefficients which leads to lower complexity and thus higher
bias.

\textbf{Q23:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{variance=}\ControlFlowTok{function}\NormalTok{(lambda,X,x0,sigma)}
\NormalTok{\{}
\NormalTok{  p=}\KeywordTok{dim}\NormalTok{(X)[}\DecValTok{2}\NormalTok{]}
\NormalTok{  inv=}\KeywordTok{solve}\NormalTok{(}\KeywordTok{t}\NormalTok{(X)}\OperatorTok{%*%}\NormalTok{X}\OperatorTok{+}\NormalTok{lambda}\OperatorTok{*}\KeywordTok{diag}\NormalTok{(p))}
\NormalTok{  W =}\StringTok{ }\NormalTok{inv}\OperatorTok{%*%}\KeywordTok{t}\NormalTok{(X)}
\NormalTok{  value=sigma}\OperatorTok{*}\NormalTok{(}\KeywordTok{t}\NormalTok{(x0)}\OperatorTok{%*%}\NormalTok{W)}\OperatorTok{%*%}\KeywordTok{t}\NormalTok{(}\KeywordTok{t}\NormalTok{(x0)}\OperatorTok{%*%}\NormalTok{W)}\OperatorTok{*}\NormalTok{sigma}
  \KeywordTok{return}\NormalTok{(value)}
\NormalTok{\}}
\NormalTok{thislambda=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DataTypeTok{length=}\DecValTok{500}\NormalTok{)}
\NormalTok{variancelambda=}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\KeywordTok{length}\NormalTok{(thislambda))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(thislambda)) variancelambda[i]=}\KeywordTok{variance}\NormalTok{(thislambda[i],X,x0,sigma)}
\KeywordTok{plot}\NormalTok{(thislambda,variancelambda,}\DataTypeTok{col=}\DecValTok{4}\NormalTok{,}\DataTypeTok{type=}\StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rmarkdown1_files/figure-latex/unnamed-chunk-15-1.pdf}

The variance curve follows our expectations in the sense that it
decreases as lambda increases i.e.~the complexity decreases.

\textbf{Q24:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tot=sqbiaslambda}\OperatorTok{+}\NormalTok{variancelambda}\OperatorTok{+}\NormalTok{sigma}\OperatorTok{^}\DecValTok{2}
\KeywordTok{which.min}\NormalTok{(tot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 249
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{thislambda[}\KeywordTok{which.min}\NormalTok{(tot)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.993988
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(thislambda,tot,}\DataTypeTok{col=}\DecValTok{1}\NormalTok{,}\DataTypeTok{type=}\StringTok{"l"}\NormalTok{,}\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\KeywordTok{max}\NormalTok{(tot)))}
\KeywordTok{lines}\NormalTok{(thislambda, sqbiaslambda,}\DataTypeTok{col=}\DecValTok{2}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(thislambda, variancelambda,}\DataTypeTok{col=}\DecValTok{4}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(thislambda,}\KeywordTok{rep}\NormalTok{(sigma}\OperatorTok{^}\DecValTok{2}\NormalTok{,}\DecValTok{500}\NormalTok{),}\DataTypeTok{col=}\StringTok{"orange"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{thislambda[}\KeywordTok{which.min}\NormalTok{(tot)],}\DataTypeTok{col=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rmarkdown1_files/figure-latex/unnamed-chunk-16-1.pdf}

The optimal value of \(\lambda\) is chosen to minimize the black curve,
which is the sum of the irriducible error, the variance and the squared
bias. The value optimizing this problem is \(\lambda = 0.993988\).


\end{document}
